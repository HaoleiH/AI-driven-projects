{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10176016,"sourceType":"datasetVersion","datasetId":6285352}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install transformers langchain langchain-huggingface langchain-community faiss-cpu huggingface-hub unstructured gradio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T21:48:24.703960Z","iopub.execute_input":"2024-12-21T21:48:24.704321Z","iopub.status.idle":"2024-12-21T21:48:55.932938Z","shell.execute_reply.started":"2024-12-21T21:48:24.704293Z","shell.execute_reply":"2024-12-21T21:48:55.931583Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\nCollecting langchain\n  Downloading langchain-0.3.13-py3-none-any.whl.metadata (7.1 kB)\nCollecting langchain-huggingface\n  Downloading langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\nCollecting langchain-community\n  Downloading langchain_community-0.3.13-py3-none-any.whl.metadata (2.9 kB)\nCollecting faiss-cpu\n  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\nRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (0.24.7)\nCollecting unstructured\n  Downloading unstructured-0.16.11-py3-none-any.whl.metadata (24 kB)\nCollecting gradio\n  Downloading gradio-5.9.1-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.35)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\nCollecting langchain-core<0.4.0,>=0.3.26 (from langchain)\n  Downloading langchain_core-0.3.28-py3-none-any.whl.metadata (6.3 kB)\nCollecting langchain-text-splitters<0.4.0,>=0.3.3 (from langchain)\n  Downloading langchain_text_splitters-0.3.4-py3-none-any.whl.metadata (2.3 kB)\nCollecting langsmith<0.3,>=0.1.17 (from langchain)\n  Downloading langsmith-0.2.4-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\nCollecting sentence-transformers>=2.6.0 (from langchain-huggingface)\n  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\nCollecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\nCollecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n  Downloading pydantic_settings-2.7.0-py3-none-any.whl.metadata (3.5 kB)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.12.2)\nRequirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\nCollecting filetype (from unstructured)\n  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\nCollecting python-magic (from unstructured)\n  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.4)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.2.4)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.3)\nRequirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.14.0)\nCollecting python-iso639 (from unstructured)\n  Downloading python_iso639-2024.10.22-py3-none-any.whl.metadata (13 kB)\nCollecting langdetect (from unstructured)\n  Downloading langdetect-1.0.9.tar.gz (981 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting rapidfuzz (from unstructured)\n  Downloading rapidfuzz-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nCollecting backoff (from unstructured)\n  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\nCollecting unstructured-client (from unstructured)\n  Downloading unstructured_client-0.28.1-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.16.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.9.5)\nCollecting python-oxmsg (from unstructured)\n  Downloading python_oxmsg-0.0.1-py3-none-any.whl.metadata (5.0 kB)\nRequirement already satisfied: html5lib in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.1)\nRequirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (22.1.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\nCollecting fastapi<1.0,>=0.115.2 (from gradio)\n  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\nCollecting ffmpy (from gradio)\n  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\nCollecting gradio-client==1.5.2 (from gradio)\n  Downloading gradio_client-1.5.2-py3-none-any.whl.metadata (7.1 kB)\nCollecting httpx>=0.24.1 (from gradio)\n  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\nCollecting huggingface-hub\n  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\nRequirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\nCollecting orjson~=3.0 (from gradio)\n  Downloading orjson-3.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.4)\nRequirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\nRequirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\nCollecting python-multipart>=0.0.18 (from gradio)\n  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\nCollecting ruff>=0.2.2 (from gradio)\n  Downloading ruff-0.8.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\nCollecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\nCollecting semantic-version~=2.0 (from gradio)\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\nCollecting starlette<1.0,>=0.40.0 (from gradio)\n  Downloading starlette-0.42.0-py3-none-any.whl.metadata (6.0 kB)\nCollecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\nCollecting uvicorn>=0.14.0 (from gradio)\n  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.2->gradio) (14.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.1)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.23.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\nCollecting starlette<1.0,>=0.40.0 (from gradio)\n  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\nCollecting httpcore==1.* (from httpx>=0.24.1->gradio)\n  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\nCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\nCollecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.26->langchain)\n  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\nCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.3,>=0.1.17->langchain)\n  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\nCollecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.4.1+cu121)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.0)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.8.1)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.6)\nRequirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from html5lib->unstructured) (1.16.0)\nRequirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib->unstructured) (0.5.1)\nRequirement already satisfied: olefile in /usr/local/lib/python3.10/dist-packages (from python-oxmsg->unstructured) (0.47)\nINFO: pip is looking at multiple versions of unstructured-client to determine which version is compatible with other requirements. This could take a while.\nCollecting unstructured-client (from unstructured)\n  Downloading unstructured_client-0.28.0-py3-none-any.whl.metadata (20 kB)\n  Downloading unstructured_client-0.27.0-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: cryptography>=3.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (43.0.1)\nRequirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (0.2.0)\nCollecting jsonpath-python<2.0.0,>=1.0.6 (from unstructured-client->unstructured)\n  Downloading jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.6.0)\nRequirement already satisfied: pypdf>=4.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (5.1.0)\nRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.1->unstructured-client->unstructured) (1.17.1)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.26->langchain) (3.0.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.5.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured) (2.22)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.0)\nDownloading langchain-0.3.13-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\nDownloading langchain_community-0.3.13-py3-none-any.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading unstructured-0.16.11-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading gradio-5.9.1-py3-none-any.whl (57.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading gradio_client-1.5.2-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.4/320.4 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.5/450.5 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\nDownloading langchain_core-0.3.28-py3-none-any.whl (411 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.3.4-py3-none-any.whl (27 kB)\nDownloading langsmith-0.2.4-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading orjson-3.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.3/131.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic_settings-2.7.0-py3-none-any.whl (29 kB)\nDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\nDownloading ruff-0.8.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\nDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading starlette-0.41.3-py3-none-any.whl (73 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\nDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\nDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\nDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\nDownloading python_iso639-2024.10.22-py3-none-any.whl (274 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.9/274.9 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\nDownloading python_oxmsg-0.0.1-py3-none-any.whl (31 kB)\nDownloading rapidfuzz-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading unstructured_client-0.27.0-py3-none-any.whl (59 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\nDownloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\nDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\nDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: langdetect\n  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993221 sha256=0220cfbe7a1e1a6efa42e55cc6f0af4ff3f7b21215edabe9d5c2f5fc899dc9a0\n  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\nSuccessfully built langdetect\nInstalling collected packages: filetype, tomlkit, semantic-version, ruff, rapidfuzz, python-oxmsg, python-multipart, python-magic, python-iso639, python-dotenv, orjson, langdetect, jsonpath-python, jsonpatch, httpx-sse, h11, ffmpy, faiss-cpu, backoff, uvicorn, starlette, requests-toolbelt, huggingface-hub, httpcore, pydantic-settings, httpx, fastapi, unstructured-client, safehttpx, langsmith, gradio-client, unstructured, sentence-transformers, langchain-core, gradio, langchain-text-splitters, langchain-huggingface, langchain, langchain-community\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.24.7\n    Uninstalling huggingface-hub-0.24.7:\n      Successfully uninstalled huggingface-hub-0.24.7\nSuccessfully installed backoff-2.2.1 faiss-cpu-1.9.0.post1 fastapi-0.115.6 ffmpy-0.5.0 filetype-1.2.0 gradio-5.9.1 gradio-client-1.5.2 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 httpx-sse-0.4.0 huggingface-hub-0.27.0 jsonpatch-1.33 jsonpath-python-1.0.6 langchain-0.3.13 langchain-community-0.3.13 langchain-core-0.3.28 langchain-huggingface-0.1.2 langchain-text-splitters-0.3.4 langdetect-1.0.9 langsmith-0.2.4 orjson-3.10.12 pydantic-settings-2.7.0 python-dotenv-1.0.1 python-iso639-2024.10.22 python-magic-0.4.27 python-multipart-0.0.20 python-oxmsg-0.0.1 rapidfuzz-3.11.0 requests-toolbelt-1.0.0 ruff-0.8.4 safehttpx-0.1.6 semantic-version-2.10.0 sentence-transformers-3.3.1 starlette-0.41.3 tomlkit-0.13.2 unstructured-0.16.11 unstructured-client-0.27.0 uvicorn-0.34.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# llm instance","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_hf = user_secrets.get_secret(\"hf_api_key\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T19:37:48.711383Z","iopub.execute_input":"2024-12-21T19:37:48.711641Z","iopub.status.idle":"2024-12-21T19:37:48.972886Z","shell.execute_reply.started":"2024-12-21T19:37:48.711617Z","shell.execute_reply":"2024-12-21T19:37:48.971644Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain_huggingface import HuggingFaceEndpoint\nfrom langchain.chains import LLMChain\nfrom langchain_core.prompts import PromptTemplate\nquestion = \"Who won the FIFA World Cup in the year 1994? \"\n\ntemplate = \"\"\"Question: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate.from_template(template)\n\nrepo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n\nllm = HuggingFaceEndpoint(\n    repo_id=repo_id,\n    max_length=512,\n    temperature=0.2,\n    huggingfacehub_api_token=secret_value_hf,\n)\n#llm_chain = prompt | llm\n#print(llm_chain.invoke({\"question\": question}))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T20:09:25.213130Z","iopub.execute_input":"2024-12-21T20:09:25.214063Z","iopub.status.idle":"2024-12-21T20:09:25.356868Z","shell.execute_reply.started":"2024-12-21T20:09:25.214027Z","shell.execute_reply":"2024-12-21T20:09:25.355476Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# embedding model","metadata":{}},{"cell_type":"code","source":"from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n\nhf_embeddings = HuggingFaceInferenceAPIEmbeddings(\n    api_key=secret_value_hf,\n    model_name=\"sentence-transformers/all-MiniLM-l6-v2\"\n)\n#texts = [\"Hello, world!\", \"How are you?\"]\n#hf_embeddings.embed_documents(texts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T20:01:11.566903Z","iopub.execute_input":"2024-12-21T20:01:11.567308Z","iopub.status.idle":"2024-12-21T20:01:11.572393Z","shell.execute_reply.started":"2024-12-21T20:01:11.567273Z","shell.execute_reply":"2024-12-21T20:01:11.571161Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# load pdf and build faiss vector database","metadata":{}},{"cell_type":"code","source":"!mkdir md_files","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T20:06:27.439124Z","iopub.execute_input":"2024-12-21T20:06:27.439556Z","iopub.status.idle":"2024-12-21T20:06:27.563197Z","shell.execute_reply.started":"2024-12-21T20:06:27.439526Z","shell.execute_reply":"2024-12-21T20:06:27.561831Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cp \"/kaggle/input/cp-papers/chalcogenide_perovskite1/1_1733975053.267784.md\" \"md_files/1.md\"\n!cp \"/kaggle/input/cp-papers/chalcogenide_perovskite2/2_1733975093.5062084.md\" \"md_files/2.md\"\n!cp \"/kaggle/input/cp-papers/chalcogenide_perovskite3/3_1733975145.179506.md\" \"md_files/3.md\"\n!cp \"/kaggle/input/cp-papers/chalcogenide_perovskite4/Adv Funct Materials - 2023 - Yu - Chalcogenide Perovskite Thin Films with Controlled Phases for Optoelectronics_1733974706.4877486.md\" \"md_files/4.md\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T20:06:29.109977Z","iopub.execute_input":"2024-12-21T20:06:29.110409Z","iopub.status.idle":"2024-12-21T20:06:29.594784Z","shell.execute_reply.started":"2024-12-21T20:06:29.110373Z","shell.execute_reply":"2024-12-21T20:06:29.593603Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain_community.document_loaders import UnstructuredMarkdownLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_huggingface import HuggingFaceEmbeddings\n#from langchain.vectorstores import FAISS\nfrom langchain_community.vectorstores import FAISS\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom langchain_huggingface.llms import HuggingFacePipeline\nfrom langchain.prompts import PromptTemplate\n\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\nimport glob\nimport gradio as gr","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T20:07:05.723715Z","iopub.execute_input":"2024-12-21T20:07:05.724071Z","iopub.status.idle":"2024-12-21T20:07:25.934230Z","shell.execute_reply.started":"2024-12-21T20:07:05.724044Z","shell.execute_reply":"2024-12-21T20:07:25.933036Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"md_path = glob.glob( \"md_files/*.md\")\n\ndocs = [UnstructuredMarkdownLoader(md).load() for md in md_path]\ndocs_list = [item for sublist in docs for item in sublist]\n\n# Split documents\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=1000, chunk_overlap=200\n)\ndoc_splits = text_splitter.split_documents(docs_list)\n\n\n# Create the embeddings + retriever\n\ndb = FAISS.from_documents(doc_splits,\n                          hf_embeddings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T20:07:38.236436Z","iopub.execute_input":"2024-12-21T20:07:38.237208Z","iopub.status.idle":"2024-12-21T20:07:46.567072Z","shell.execute_reply.started":"2024-12-21T20:07:38.237174Z","shell.execute_reply":"2024-12-21T20:07:46.566052Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# retrieval and generation","metadata":{}},{"cell_type":"code","source":"prompt_template = '''You are an assistant for question-answering tasks. \n        Here is the context to use to answer the question:\n        {context} \n        Think carefully about the above context. \n        Now, review the user question:\n        {question}\n        Provide an answer to this questions using only the above context. \n        keep the answer concise.\n        Answer:'''\n\nprompt = PromptTemplate(\n    input_variables=[\"context\", \"question\"],\n    template=prompt_template,\n)\n\n\nllm_chain = prompt | llm | StrOutputParser()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T20:09:37.198973Z","iopub.execute_input":"2024-12-21T20:09:37.199378Z","iopub.status.idle":"2024-12-21T20:09:37.205123Z","shell.execute_reply.started":"2024-12-21T20:09:37.199344Z","shell.execute_reply":"2024-12-21T20:09:37.203852Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"retriever = db.as_retriever()\n\nrag_chain = (\n {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | llm_chain\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T20:10:03.148924Z","iopub.execute_input":"2024-12-21T20:10:03.149327Z","iopub.status.idle":"2024-12-21T20:10:03.154897Z","shell.execute_reply.started":"2024-12-21T20:10:03.149292Z","shell.execute_reply":"2024-12-21T20:10:03.153521Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#questions=\"what is advantage of BaZrS3?\"\n#rag_chain.invoke(questions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T20:12:44.050817Z","iopub.execute_input":"2024-12-21T20:12:44.051274Z","iopub.status.idle":"2024-12-21T20:12:44.055695Z","shell.execute_reply.started":"2024-12-21T20:12:44.051239Z","shell.execute_reply":"2024-12-21T20:12:44.054513Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# gradio interface","metadata":{}},{"cell_type":"code","source":"from langchain_community.document_loaders import UnstructuredMarkdownLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_huggingface import HuggingFaceEmbeddings\n#from langchain.vectorstores import FAISS\nfrom langchain_community.vectorstores import FAISS\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom langchain_huggingface.llms import HuggingFacePipeline\nfrom langchain.prompts import PromptTemplate\n\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\nimport glob\nimport gradio as gr\n\nfrom langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n\nhf_embeddings = HuggingFaceInferenceAPIEmbeddings(\n    api_key=secret_value_hf,\n    model_name=\"sentence-transformers/all-MiniLM-l6-v2\"\n)\n\nmd_path = glob.glob( \"md_files/*.md\")\n\ndocs = [UnstructuredMarkdownLoader(md).load() for md in md_path]\ndocs_list = [item for sublist in docs for item in sublist]\n\n# Split documents\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=1000, chunk_overlap=200\n)\ndoc_splits = text_splitter.split_documents(docs_list)\n\n\n# Create the embeddings + retriever\n\ndb = FAISS.from_documents(doc_splits,\n                          hf_embeddings)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# prompt\nprompt_template = '''You are an assistant for question-answering tasks. \n        Here is the context to use to answer the question:\n        {context} \n        Think carefully about the above context. \n        Now, review the user question:\n        {question}\n        Provide an answer to this questions using only the above context. \n        Use three sentences maximum and keep the answer concise.\n        Answer:'''\n\nprompt = PromptTemplate(\n    input_variables=[\"context\", \"question\"],\n    template=prompt_template,\n)\n\n\n# gradio interface\n\ndef get_output(model_name:str,is_RAG:str,questions:str):\n    if model_name==\"mistralai/Mistral-7B-Instruct-v0.2\":\n        #repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n        llm = HuggingFaceEndpoint(\n            repo_id=model_name,\n            max_length=512,\n            temperature=0.2,\n            huggingfacehub_api_token=secret_value_hf,\n        )\n        llm_chain = prompt | llm | StrOutputParser()\n        retriever = db.as_retriever(\n            search_type=\"similarity\",\n            search_kwargs={'k': 4}\n        )\n        \n        rag_chain = (\n         {\"context\": retriever, \"question\": RunnablePassthrough()}\n            | llm_chain\n        )\n        if is_RAG== \"RAG\":\n            generation2=rag_chain.invoke(questions)\n            return generation2\n        else:\n            generation1=llm_chain.invoke({\"context\":\"\", \"question\": questions})\n            return generation1\n    elif model_name==\"meta-llama/Llama-3.2-3B-Instruct\":\n        llm = HuggingFaceEndpoint(\n            repo_id=model_name,\n            max_length=512,\n            temperature=0.2,\n            huggingfacehub_api_token=secret_value_hf,\n        )\n        llm_chain = prompt | llm | StrOutputParser()\n        retriever = db.as_retriever()\n        \n        rag_chain = (\n         {\"context\": retriever, \"question\": RunnablePassthrough()}\n            | llm_chain\n        )\n        if is_RAG== \"RAG\":\n            generation2=rag_chain.invoke(questions)\n            return generation2\n        else:\n            generation1=llm_chain.invoke({\"context\":\"\", \"question\": questions})\n            return generation1\n    elif model_name==\"Qwen/Qwen2.5-72B-Instruct\":\n        llm = HuggingFaceEndpoint(\n            repo_id=model_name,\n            max_length=512,\n            temperature=0.2,\n            huggingfacehub_api_token=secret_value_hf,\n        )\n        llm_chain = prompt | llm | StrOutputParser()\n        retriever = db.as_retriever()\n        \n        rag_chain = (\n         {\"context\": retriever, \"question\": RunnablePassthrough()}\n            | llm_chain\n        )\n        if is_RAG== \"RAG\":\n            generation2=rag_chain.invoke(questions)\n            return generation2\n        else:\n            generation1=llm_chain.invoke({\"context\":\"\", \"question\": questions})\n            return generation1\n\ndemo = gr.Interface(\n    fn=get_output,\n    inputs=[\n        gr.Radio(\n            choices=[\"mistralai/Mistral-7B-Instruct-v0.2\", \"meta-llama/Llama-3.2-3B-Instruct\",\"Qwen/Qwen2.5-72B-Instruct\"],\n            type=\"value\",\n            value=\"mistralai/Mistral-7B-Instruct-v0.2\",  # Set default value to \"Model 1\"\n            label=\"model name\"\n        ),\n        gr.Radio(\n            choices=[\"RAG\", \"No RAG\"],\n            type=\"value\",\n            value=\"RAG\",  # Set default value to \"Model 1\"\n            label=\"RAG or not\"\n        ),\n        gr.Textbox(label=\"Input Questions\")#,info=\"input questions on chalcogenide perovskites\")\n    ],\n    outputs=\"markdown\",\n    title=\"Ask questions on chalcogenide perovskites\",\n    description=\"\"\"\n    ## this space is implemented by Retrieval-Augmented Generation of Large Language Models , based haolei Hui's work on chalcogenide perovskite papers.\n    \"\"\",\n    flagging_mode=\"never\",\n    examples=[[\"mistralai/Mistral-7B-Instruct-v0.2\",\"RAG\",\"what is advantage of BaZrS3?\"],\n              [\"mistralai/Mistral-7B-Instruct-v0.2\",\"RAG\",\"what is bandgap of SrHfS3?\"],\n              [\"mistralai/Mistral-7B-Instruct-v0.2\",\"RAG\",\"why is chalcogenide perovskite important?\"]\n              ]\n)\n\n# Launch the Gradio app\nif __name__ == \"__main__\":\n    demo.launch(share=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T21:04:12.633092Z","iopub.execute_input":"2024-12-21T21:04:12.633500Z","iopub.status.idle":"2024-12-21T21:04:13.486666Z","shell.execute_reply.started":"2024-12-21T21:04:12.633472Z","shell.execute_reply":"2024-12-21T21:04:13.485441Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Custom CSS to style the output area\ncustom_css = \"\"\"\n#output_area {\n    background-color: #1e1e1e; /* Dark background */\n    color: #ffffff; /* White text */\n    padding: 10px;\n    border-radius: 5px;\n    border: 1px solid #333333; /* Dark border */\n    margin-top: 10px;\n}\n\n#output_area h3 {\n    color: #ffcc00; /* Yellow title color */\n    margin-bottom: 10px;\n}\n\"\"\"\n\nwith gr.Blocks(title=\"Ask Questions on Chalcogenide Perovskites\",theme=gr.themes.Ocean(),css=custom_css) as demo:\n    gr.Markdown(\"\"\"\n    ## Retrieval-Augmented Generation for Chalcogenide Perovskites\n    This space implements Retrieval-Augmented Generation (RAG) using large language models, based on Hui Haolei's work on chalcogenide perovskite papers. You can select different models and choose whether to use RAG to enhance the responses.\n    For details of this project, check my [github](https://github.com/HaoleiH/AI-driven-projects).\n    \"\"\")\n    \n    with gr.Row():\n        model_name = gr.Radio(\n            choices=[\"mistralai/Mistral-7B-Instruct-v0.2\", \"meta-llama/Llama-3.2-3B-Instruct\", \"Qwen/Qwen2.5-72B-Instruct\"],\n            value=\"mistralai/Mistral-7B-Instruct-v0.2\",\n            label=\"Model Name\",\n            info=\"Select the model you want to use.\"\n        )\n    \n    with gr.Row():\n        rag = gr.Radio(\n            choices=[\"RAG\", \"No RAG\"],\n            value=\"RAG\",\n            label=\"RAG or Not\",\n            info=\"Choose whether to use Retrieval-Augmented Generation.\"\n        )\n    \n    with gr.Row():\n        question = gr.Textbox(\n            label=\"Input Question\",\n            placeholder=\"Enter your question about chalcogenide perovskites here...\",\n            lines=2  # Increase the number of lines for better input experience\n        )\n    \n    with gr.Row():\n        submit_button = gr.Button(\"Submit\")\n    \n    with gr.Row():\n        output = gr.Markdown(label=\"Answer\",\n                            #lines=10,  # Increase the number of lines for the output area\n                            elem_id=\"output_area\"  # Assign a custom ID for styling\n                            )\n    submit_button.click(\n        fn=get_output,\n        inputs=[model_name, rag, question],\n        outputs=output\n    )\n\n    gr.Examples(\n        examples=[\n            [\"mistralai/Mistral-7B-Instruct-v0.2\", \"RAG\", \"What is the advantage of BaZrS3?\"],\n            [\"mistralai/Mistral-7B-Instruct-v0.2\", \"RAG\", \"What is the bandgap of SrHfS3?\"],\n            [\"mistralai/Mistral-7B-Instruct-v0.2\", \"RAG\", \"Why is chalcogenide perovskite important?\"]\n        ],\n        fn=get_output,\n        inputs=[model_name, rag, question],\n        outputs=output\n    )\n\ndemo.launch()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T21:34:08.294027Z","iopub.execute_input":"2024-12-21T21:34:08.294467Z","iopub.status.idle":"2024-12-21T21:34:09.188088Z","shell.execute_reply.started":"2024-12-21T21:34:08.294433Z","shell.execute_reply":"2024-12-21T21:34:09.187211Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Final  code","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_hf = user_secrets.get_secret(\"hf_api_key\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T21:50:31.858305Z","iopub.execute_input":"2024-12-21T21:50:31.858666Z","iopub.status.idle":"2024-12-21T21:50:32.071401Z","shell.execute_reply.started":"2024-12-21T21:50:31.858639Z","shell.execute_reply":"2024-12-21T21:50:32.070222Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"!mkdir md_files\n!cp \"/kaggle/input/cp-papers/chalcogenide_perovskite1/1_1733975053.267784.md\" \"md_files/1.md\"\n!cp \"/kaggle/input/cp-papers/chalcogenide_perovskite2/2_1733975093.5062084.md\" \"md_files/2.md\"\n!cp \"/kaggle/input/cp-papers/chalcogenide_perovskite3/3_1733975145.179506.md\" \"md_files/3.md\"\n!cp \"/kaggle/input/cp-papers/chalcogenide_perovskite4/Adv Funct Materials - 2023 - Yu - Chalcogenide Perovskite Thin Films with Controlled Phases for Optoelectronics_1733974706.4877486.md\" \"md_files/4.md\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T21:50:54.151884Z","iopub.execute_input":"2024-12-21T21:50:54.152220Z","iopub.status.idle":"2024-12-21T21:50:54.833028Z","shell.execute_reply.started":"2024-12-21T21:50:54.152195Z","shell.execute_reply":"2024-12-21T21:50:54.831243Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from langchain_community.document_loaders import UnstructuredMarkdownLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_huggingface import HuggingFaceEmbeddings\n#from langchain.vectorstores import FAISS\nfrom langchain_community.vectorstores import FAISS\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom langchain_huggingface.llms import HuggingFacePipeline\nfrom langchain.prompts import PromptTemplate\n\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_huggingface import HuggingFaceEndpoint\nimport glob\nimport gradio as gr\n\nfrom langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n\nhf_embeddings = HuggingFaceInferenceAPIEmbeddings(\n    api_key=secret_value_hf,\n    model_name=\"sentence-transformers/all-MiniLM-l6-v2\"\n)\n\nmd_path = glob.glob( \"md_files/*.md\")\n\ndocs = [UnstructuredMarkdownLoader(md).load() for md in md_path]\ndocs_list = [item for sublist in docs for item in sublist]\n\n# Split documents\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=1000, chunk_overlap=200\n)\ndoc_splits = text_splitter.split_documents(docs_list)\n\n\n# Create the embeddings + retriever\n\ndb = FAISS.from_documents(doc_splits,\n                          hf_embeddings)\n\n\n# prompt\nprompt_template = '''You are an assistant for question-answering tasks. \n        Here is the context to use to answer the question:\n        {context} \n        Think carefully about the above context. \n        Now, review the user question:\n        {question}\n        Provide an answer to this questions using only the above context. \n        Use three sentences maximum and keep the answer concise.\n        Answer:'''\n\nprompt = PromptTemplate(\n    input_variables=[\"context\", \"question\"],\n    template=prompt_template,\n)\n\n\n# gradio interface\n\ndef get_output(model_name:str,is_RAG:str,questions:str):\n    if model_name==\"mistralai/Mistral-7B-Instruct-v0.2\":\n        #repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n        llm = HuggingFaceEndpoint(\n            repo_id=model_name,\n            max_length=512,\n            temperature=0.2,\n            huggingfacehub_api_token=secret_value_hf,\n        )\n        llm_chain = prompt | llm | StrOutputParser()\n        retriever = db.as_retriever(\n            search_type=\"similarity\",\n            search_kwargs={'k': 4}\n        )\n        \n        rag_chain = (\n         {\"context\": retriever, \"question\": RunnablePassthrough()}\n            | llm_chain\n        )\n        if is_RAG== \"RAG\":\n            generation2=rag_chain.invoke(questions)\n            return generation2\n        else:\n            generation1=llm_chain.invoke({\"context\":\"\", \"question\": questions})\n            return generation1\n    elif model_name==\"meta-llama/Llama-3.2-3B-Instruct\":\n        llm = HuggingFaceEndpoint(\n            repo_id=model_name,\n            max_length=512,\n            temperature=0.2,\n            huggingfacehub_api_token=secret_value_hf,\n        )\n        llm_chain = prompt | llm | StrOutputParser()\n        retriever = db.as_retriever()\n        \n        rag_chain = (\n         {\"context\": retriever, \"question\": RunnablePassthrough()}\n            | llm_chain\n        )\n        if is_RAG== \"RAG\":\n            generation2=rag_chain.invoke(questions)\n            return generation2\n        else:\n            generation1=llm_chain.invoke({\"context\":\"\", \"question\": questions})\n            return generation1\n    elif model_name==\"Qwen/Qwen2.5-72B-Instruct\":\n        llm = HuggingFaceEndpoint(\n            repo_id=model_name,\n            max_length=512,\n            temperature=0.2,\n            huggingfacehub_api_token=secret_value_hf,\n        )\n        llm_chain = prompt | llm | StrOutputParser()\n        retriever = db.as_retriever()\n        \n        rag_chain = (\n         {\"context\": retriever, \"question\": RunnablePassthrough()}\n            | llm_chain\n        )\n        if is_RAG== \"RAG\":\n            generation2=rag_chain.invoke(questions)\n            return generation2\n        else:\n            generation1=llm_chain.invoke({\"context\":\"\", \"question\": questions})\n            return generation1\n\n\n\n# Custom CSS to style the output area\ncustom_css = \"\"\"\n#output_area {\n    background-color: #1e1e1e; /* Dark background */\n    color: #ffffff; /* White text */\n    padding: 10px;\n    border-radius: 5px;\n    border: 1px solid #333333; /* Dark border */\n    margin-top: 10px;\n}\n\n#output_area h3 {\n    color: #ffcc00; /* Yellow title color */\n    margin-bottom: 10px;\n}\n\"\"\"\n\nwith gr.Blocks(title=\"Ask Questions on Chalcogenide Perovskites\",theme=gr.themes.Ocean(),css=custom_css) as demo:\n    gr.Markdown(\"\"\"\n    ## Retrieval-Augmented Generation for Chalcogenide Perovskites\n    This space implements Retrieval-Augmented Generation (RAG) using large language models, based on Hui Haolei's work on chalcogenide perovskite papers. You can select different models and choose whether to use RAG to enhance the responses.\n    \"\"\")\n    \n    with gr.Row():\n        model_name = gr.Radio(\n            choices=[\"mistralai/Mistral-7B-Instruct-v0.2\", \"meta-llama/Llama-3.2-3B-Instruct\", \"Qwen/Qwen2.5-72B-Instruct\"],\n            value=\"mistralai/Mistral-7B-Instruct-v0.2\",\n            label=\"Model Name\",\n            info=\"Select the model you want to use.\"\n        )\n    \n    with gr.Row():\n        rag = gr.Radio(\n            choices=[\"RAG\", \"No RAG\"],\n            value=\"RAG\",\n            label=\"RAG or Not\",\n            info=\"Choose whether to use Retrieval-Augmented Generation.\"\n        )\n    \n    with gr.Row():\n        question = gr.Textbox(\n            label=\"Input Question\",\n            placeholder=\"Enter your question about chalcogenide perovskites here...\",\n            lines=2  # Increase the number of lines for better input experience\n        )\n    \n    with gr.Row():\n        submit_button = gr.Button(\"Submit\")\n    \n    with gr.Row():\n        output = gr.Textbox(label=\"Response\",\n                            lines=10,  # Increase the number of lines for the output area\n                            elem_id=\"output_area\"  # Assign a custom ID for styling\n                            )\n    submit_button.click(\n        fn=get_output,\n        inputs=[model_name, rag, question],\n        outputs=output\n    )\n\n    gr.Examples(\n        examples=[\n            [\"mistralai/Mistral-7B-Instruct-v0.2\", \"RAG\", \"What is the advantage of BaZrS3?\"],\n            [\"mistralai/Mistral-7B-Instruct-v0.2\", \"RAG\", \"What is the bandgap of SrHfS3?\"],\n            [\"mistralai/Mistral-7B-Instruct-v0.2\", \"RAG\", \"Why is chalcogenide perovskite important?\"]\n        ],\n        fn=get_output,\n        inputs=[model_name, rag, question],\n        outputs=output\n    )\n\ndemo.launch()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T22:04:19.647698Z","iopub.execute_input":"2024-12-21T22:04:19.648112Z","iopub.status.idle":"2024-12-21T22:04:24.473614Z","shell.execute_reply.started":"2024-12-21T22:04:19.648083Z","shell.execute_reply":"2024-12-21T22:04:24.472619Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7861\nKaggle notebooks require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://a038d42a34d9e9bee3.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://a038d42a34d9e9bee3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}