{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with LLAMA and FAISS\n",
    "credit: https://www.matillion.com/blog/step-by-step-guide-building-a-rag-model-with-open-source-llm-llama-2-and-vector-store-faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## install packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#!rm \"pdf_files/1.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:35:47.669946Z",
     "iopub.status.busy": "2024-12-14T04:35:47.669530Z",
     "iopub.status.idle": "2024-12-14T04:35:47.677342Z",
     "shell.execute_reply": "2024-12-14T04:35:47.676354Z",
     "shell.execute_reply.started": "2024-12-14T04:35:47.669911Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['md_files\\\\1_1733975053.267784.md', 'md_files\\\\2_1733975093.5062084.md', 'md_files\\\\3_1733975145.179506.md', 'md_files\\\\Adv Funct Materials - 2023 - Yu - Chalcogenide Perovskite Thin Films with Controlled Phases for Optoelectronics_1733974706.4877486.md']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import glob\n",
    "md_folder=\"md_files\"\n",
    "md_path = glob.glob(md_folder + \"/*.md\")\n",
    "print(md_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: md_files\\1_1733975053.267784.md\n",
      "Processed: md_files\\1_1733975053.267784.md\n",
      "Processed: md_files\\1_1733975053.267784.md\n",
      "Processed: md_files\\1_1733975053.267784.md\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import markdown\n",
    "#from pdfminer.high_level import extract_text as extract_text_from_pdf\n",
    "from io import StringIO\n",
    "from html.parser import HTMLParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "chunk_size = 1200\n",
    "chunk_overlap = 300\n",
    "\n",
    "# List to store all document chunks\n",
    "all_docs = []\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs = True\n",
    "        self.text = StringIO()\n",
    "\n",
    "    def handle_data(self, d):\n",
    "        self.text.write(d)\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.text.getvalue()\n",
    "\n",
    "def clean_markdown(text):\n",
    "    \"\"\"Clean Markdown syntax from text.\"\"\"\n",
    "    # Remove Markdown URL links\n",
    "    text = re.sub(r'\\[([^\\]]+)\\]\\([^)]+\\)', r'\\1', text)\n",
    "    # Remove bold and italic text markers\n",
    "    text = re.sub(r'\\*\\*([^*]+)\\*\\*', r'\\1', text)\n",
    "    text = re.sub(r'\\*([^*]+)\\*', r'\\1', text)\n",
    "    text = re.sub(r'__([^_]+)__', r'\\1', text)\n",
    "    text = re.sub(r'_([^_]+)_', r'\\1', text)\n",
    "    # Remove images and their references\n",
    "    text = re.sub(r'!\\[[^\\]]]*]\\([^)]*\\)', '', text)\n",
    "    # Remove headers markers\n",
    "    text = re.sub(r'#+\\s?', '', text)\n",
    "    # Remove other Markdown syntax as needed (e.g., tables, bullet points)\n",
    "    text = re.sub(r'\\|', ' ', text)\n",
    "    text = re.sub(r'-{2,}', '', text)\n",
    "    text = re.sub(r'\\n{2,}', '\\n', text)  # Remove extra newlines\n",
    "    return text\n",
    "\n",
    "def extract_text_from_md(md_path):\n",
    "    \"\"\"Extract and clean text from a Markdown file.\"\"\"\n",
    "    with open(md_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        md_content = file.read()\n",
    "        html = markdown.markdown(md_content)\n",
    "        text = strip_tags(html)\n",
    "        return clean_markdown(text)\n",
    "def strip_tags(html):\n",
    "    \"\"\"Remove HTML tags from a string.\"\"\"\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "\n",
    "for file_path in md_path:\n",
    "    file_content = extract_text_from_md(file_path)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    docs = text_splitter.split_text(file_content)\n",
    "    file_name_without_extension = os.path.splitext(filename)[0]\n",
    "    for i, chunk in enumerate(docs):\n",
    "        # Define metadata for each chunk (you can customize this)\n",
    "        metadata = {\n",
    "            \"File Name\": file_name_without_extension,\n",
    "            \"Chunk Number\": i + 1,\n",
    "        }\n",
    "        # Create a header with metadata and file name\n",
    "        header = f\"File Name: {file_name_without_extension}\\n\"\n",
    "        for key, value in metadata.items():\n",
    "            header += f\"{key}: {value}\\n\"\n",
    "        \n",
    "        # Combine header, file name, and chunk content\n",
    "        chunk_with_header = header + file_name_without_extension + \"\\n\" + chunk\n",
    "        all_docs.append(chunk_with_header)\n",
    "    print(f\"Processed: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create FAISS vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:36:10.912637Z",
     "iopub.status.busy": "2024-12-14T04:36:10.911814Z",
     "iopub.status.idle": "2024-12-14T04:37:18.509963Z",
     "shell.execute_reply": "2024-12-14T04:37:18.508936Z",
     "shell.execute_reply.started": "2024-12-14T04:36:10.912595Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "#from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# Initialize HuggingFaceInstructEmbeddings\n",
    "'''\n",
    "embeding_name = \"/kaggle/input/mistral/pytorch/7b-v0.1-hf/1\"#\"/kaggle/input/gemma-2/pytorch/gemma-2-2b-pt/1\"#\"hkunlp/instructor-large\"\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "hf_embedding = HuggingFaceEmbeddings(\n",
    "    model_name=embeding_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "'''\n",
    "# use ollama embedding\n",
    "local_llm = \"llama3.2:3b-instruct-fp16\"\n",
    "embedding=OllamaEmbeddings(model=local_llm)\n",
    "# Embed and index all the documents using FAISS\n",
    "db = FAISS.from_texts(all_docs, embedding)\n",
    "\n",
    "# Save the indexed data locally\n",
    "db.save_local(\"faiss_AiDoc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the FAISS index from local storage\n",
    "#db = FAISS.load_local(\"faiss_AiDoc\", embeddings=hf_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:37:30.558138Z",
     "iopub.status.busy": "2024-12-14T04:37:30.557384Z",
     "iopub.status.idle": "2024-12-14T04:37:38.668415Z",
     "shell.execute_reply": "2024-12-14T04:37:38.667477Z",
     "shell.execute_reply.started": "2024-12-14T04:37:30.558098Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "import transformers\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import (\n",
    "    StreamingStdOutCallbackHandler\n",
    ")\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Template for question-answer prompt\n",
    "template = \"\"\"Question: {question} \\n\\nAnswer:\"\"\"\n",
    "# Initialize prompt template and callback manager\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "# Create LLMChain\n",
    "#llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "llm_chain = prompt | llm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define a query to search the indexed documents\n",
    "query = \"what is advantage of BaZrS3?\"\n",
    "# Search for semantically similar chunks and return the top 5 chunks\n",
    "search = db.similarity_search(query, k=2)\n",
    "\n",
    "# Define a template for generating a final prompt\n",
    "template = '''Context: {context}\n",
    "Based on the Context, please answer the following question:\n",
    "Question: {question}\n",
    "Provide an answer based on the context only, without using general knowledge. The answer\n",
    "should be derived directly from the context provided.\n",
    "Please correct any grammatical errors for improved readability.\n",
    "If the context does not contain relevant information to answer the question, state that the\n",
    "answer is not available in the given context.\n",
    "Please include the source title of the information as a reference of how you arrive at your\n",
    "answer. '''\n",
    "\n",
    "# Create a prompt template\n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)\n",
    "\n",
    "# Format the final prompt with the query and search results\n",
    "final_prompt = prompt.format(question=query, context=search)\n",
    "\n",
    "# Run LLMChain to generate an answer based on the context\n",
    "generation=llm_chain.invoke(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, one advantage of BaZrS3 can be inferred:\\n\\nBaZrS3 has reasonably long carrier lifetimes, reaching approximately 50 ns as measured by time-resolved PL.\\n\\nThis suggests that BaZrS3 has a favorable property related to its carrier lifetime, which is beneficial for certain applications.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gradio interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\haoze\\anaconda3\\envs\\RAG\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: md_files\\1_1733975053.267784.md\n",
      "Processed: md_files\\2_1733975093.5062084.md\n",
      "Processed: md_files\\3_1733975145.179506.md\n",
      "Processed: md_files\\Adv Funct Materials - 2023 - Yu - Chalcogenide Perovskite Thin Films with Controlled Phases for Optoelectronics_1733974706.4877486.md\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain.vectorstores import FAISS\n",
    "#from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings,ChatOllama\n",
    "from langchain import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain import PromptTemplate\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import (\n",
    "    StreamingStdOutCallbackHandler\n",
    ")\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "import glob\n",
    "\n",
    "import glob\n",
    "md_folder=\"md_files\"\n",
    "md_path = glob.glob(md_folder + \"/*.md\")\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "import markdown\n",
    "#from pdfminer.high_level import extract_text as extract_text_from_pdf\n",
    "from io import StringIO\n",
    "from html.parser import HTMLParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "chunk_size = 1200\n",
    "chunk_overlap = 300\n",
    "\n",
    "# List to store all document chunks\n",
    "all_docs = []\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs = True\n",
    "        self.text = StringIO()\n",
    "\n",
    "    def handle_data(self, d):\n",
    "        self.text.write(d)\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.text.getvalue()\n",
    "\n",
    "def clean_markdown(text):\n",
    "    \"\"\"Clean Markdown syntax from text.\"\"\"\n",
    "    # Remove Markdown URL links\n",
    "    text = re.sub(r'\\[([^\\]]+)\\]\\([^)]+\\)', r'\\1', text)\n",
    "    # Remove bold and italic text markers\n",
    "    text = re.sub(r'\\*\\*([^*]+)\\*\\*', r'\\1', text)\n",
    "    text = re.sub(r'\\*([^*]+)\\*', r'\\1', text)\n",
    "    text = re.sub(r'__([^_]+)__', r'\\1', text)\n",
    "    text = re.sub(r'_([^_]+)_', r'\\1', text)\n",
    "    # Remove images and their references\n",
    "    text = re.sub(r'!\\[[^\\]]]*]\\([^)]*\\)', '', text)\n",
    "    # Remove headers markers\n",
    "    text = re.sub(r'#+\\s?', '', text)\n",
    "    # Remove other Markdown syntax as needed (e.g., tables, bullet points)\n",
    "    text = re.sub(r'\\|', ' ', text)\n",
    "    text = re.sub(r'-{2,}', '', text)\n",
    "    text = re.sub(r'\\n{2,}', '\\n', text)  # Remove extra newlines\n",
    "    return text\n",
    "\n",
    "def extract_text_from_md(md_path):\n",
    "    \"\"\"Extract and clean text from a Markdown file.\"\"\"\n",
    "    with open(md_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        md_content = file.read()\n",
    "        html = markdown.markdown(md_content)\n",
    "        text = strip_tags(html)\n",
    "        return clean_markdown(text)\n",
    "def strip_tags(html):\n",
    "    \"\"\"Remove HTML tags from a string.\"\"\"\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "\n",
    "for file_path in md_path:\n",
    "    file_content = extract_text_from_md(file_path)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    docs = text_splitter.split_text(file_content)\n",
    "    file_name_without_extension = os.path.splitext(file_path)[0]\n",
    "    for i, chunk in enumerate(docs):\n",
    "        # Define metadata for each chunk (you can customize this)\n",
    "        metadata = {\n",
    "            \"File Name\": file_name_without_extension,\n",
    "            \"Chunk Number\": i + 1,\n",
    "        }\n",
    "        # Create a header with metadata and file name\n",
    "        header = f\"File Name: {file_name_without_extension}\\n\"\n",
    "        for key, value in metadata.items():\n",
    "            header += f\"{key}: {value}\\n\"\n",
    "        \n",
    "        # Combine header, file name, and chunk content\n",
    "        chunk_with_header = header + file_name_without_extension + \"\\n\" + chunk\n",
    "        all_docs.append(chunk_with_header)\n",
    "    print(f\"Processed: {file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "local_llm = \"nomic-embed-text:latest\"\n",
    "embedding=OllamaEmbeddings(model=local_llm)\n",
    "# Embed and index all the documents using FAISS\n",
    "db = FAISS.from_texts(all_docs, embedding)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "def get_output(modelname:str,is_RAG:str,questions:str):\n",
    "    if is_RAG== \"RAG\":\n",
    "        query = questions#\"what is advantage of chalcogenide perovskite?\"\n",
    "        # Search for semantically similar chunks and return the top 5 chunks\n",
    "        search = db.similarity_search(query, k=5)\n",
    "        # Template for question-answer prompt\n",
    "        template = \"\"\"Question: {question} \\n\\nAnswer:\"\"\"\n",
    "        # Initialize prompt template and callback manager\n",
    "        prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "        llm = ChatOllama(model=modelname, temperature=0)\n",
    "        llm_chain = prompt | llm\n",
    "        \n",
    "        # Define a template for generating a final prompt\n",
    "        template = '''You are an assistant for question-answering tasks. \n",
    "\n",
    "        Here is the context to use to answer the question:\n",
    "\n",
    "        {context} \n",
    "\n",
    "        Think carefully about the above context. \n",
    "\n",
    "        Now, review the user question:\n",
    "\n",
    "        {question}\n",
    "\n",
    "        Provide an answer to this questions using only the above context. \n",
    "\n",
    "        Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "        Answer:'''\n",
    "\n",
    "\n",
    "        # Run LLMChain to generate an answer based on the context\n",
    "        rag_prompt_formatted = template.format(context=search, question=query)\n",
    "        generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "        return generation.content\n",
    "    else:\n",
    "        query = questions\n",
    "        template = \"\"\"Answer the Question: {question} \"\"\"\n",
    "        # Initialize prompt template and callback manager\n",
    "        prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "        llm = ChatOllama(model=modelname, temperature=0)\n",
    "        llm_chain = prompt | llm\n",
    "        no_rag_prompt=template.format(question=query)\n",
    "        generation=llm_chain.invoke(no_rag_prompt)\n",
    "        return generation.content\n",
    "demo = gr.Interface(\n",
    "    fn=get_output,\n",
    "    inputs=[\n",
    "        gr.Radio(\n",
    "            choices=[\"llama3.2:3b-instruct-fp16\", \"qwen2.5-coder:14b\", \"llama3.2-vision:latest\"],\n",
    "            type=\"value\",\n",
    "            value=\"llama3.2:3b-instruct-fp16\",  # Set default value to \"Model 1\"\n",
    "            label=\"Select Model\"\n",
    "        ),\n",
    "        gr.Radio(\n",
    "            choices=[\"RAG\", \"No RAG\"],\n",
    "            type=\"value\",\n",
    "            value=\"RAG\",  # Set default value to \"Model 1\"\n",
    "            label=\"RAG or not\"\n",
    "        ),\n",
    "        gr.Textbox(label=\"Input Questions\",info=\"input questions on chalcogenide perovskites\")\n",
    "    ],\n",
    "    outputs=\"markdown\",\n",
    "    title=\"RAG Llama3.2-3b based on chalcogenide perovskite papers\",\n",
    "    description=\"\"\"\n",
    "    ## Select a model, and ask a question to get the output; or just click on the examples below.\n",
    "    \"\"\",\n",
    "    examples=[[\"llama3.2:3b-instruct-fp16\",\"RAG\",\"what is advantage of BaZrS3?\"],\n",
    "              [\"qwen2.5-coder:14b\",\"RAG\",\"what is bandgap of SrHfS3?\"],\n",
    "              [\"llama3.2-vision:latest\",\"RAG\",\"why is chalcogenide perovskite important?\"]\n",
    "              ]\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6285352,
     "sourceId": 10176016,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 121027,
     "modelInstanceId": 100936,
     "sourceId": 120005,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 76277,
     "modelInstanceId": 72253,
     "sourceId": 104625,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 1902,
     "modelInstanceId": 3899,
     "sourceId": 5111,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
